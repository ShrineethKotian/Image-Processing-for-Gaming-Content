{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9383649,"sourceType":"datasetVersion","datasetId":5692975},{"sourceId":10627093,"sourceType":"datasetVersion","datasetId":6579801},{"sourceId":10627198,"sourceType":"datasetVersion","datasetId":6579867},{"sourceId":10985202,"sourceType":"datasetVersion","datasetId":6837001},{"sourceId":11016495,"sourceType":"datasetVersion","datasetId":6859309},{"sourceId":11151331,"sourceType":"datasetVersion","datasetId":6957256},{"sourceId":11165699,"sourceType":"datasetVersion","datasetId":6967795},{"sourceId":11518986,"sourceType":"datasetVersion","datasetId":7224222}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nfrom matplotlib import pyplot as plt\nfrom PIL import Image\nimport numpy as np\nimport cv2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T22:07:32.867415Z","iopub.execute_input":"2025-07-15T22:07:32.867734Z","iopub.status.idle":"2025-07-15T22:07:32.871391Z","shell.execute_reply.started":"2025-07-15T22:07:32.867708Z","shell.execute_reply":"2025-07-15T22:07:32.870633Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def load_img (filename, norm=True,):\n    img = np.array(Image.open(filename).convert(\"RGB\"))\n    if norm:   \n        img = img / 255.\n        img = img.astype(np.float32)\n    return img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T22:07:34.200496Z","iopub.execute_input":"2025-07-15T22:07:34.200767Z","iopub.status.idle":"2025-07-15T22:07:34.204971Z","shell.execute_reply.started":"2025-07-15T22:07:34.200745Z","shell.execute_reply":"2025-07-15T22:07:34.204108Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# -------------------------\n# Candidate Color Grading Options\n# -------------------------\n\ncolor_grades = {\n    \"cinematic_warm\": {\n        \"wb\": np.array([1.10, 1.00, 0.90]),\n        \"ccm\": np.array([\n            [ 1.05, -0.02, -0.03],\n            [-0.01,  1.08, -0.07],\n            [ 0.00, -0.05,  1.05],\n        ])\n    },\n    \"cool_high_contrast\": {\n        \"wb\": np.array([0.95, 1.00, 1.10]),\n        \"ccm\": np.array([\n            [ 1.10, -0.05, -0.05],\n            [-0.05,  1.20, -0.05],\n            [-0.05, -0.05,  1.10],\n        ])\n    },\n    \"vibrant_boost\": {\n        \"wb\": np.array([1.00, 1.05, 1.00]),\n        \"ccm\": np.array([\n            [ 1.20, -0.10, -0.10],\n            [-0.10,  1.20, -0.10],\n            [-0.10, -0.10,  1.20],\n        ])\n    },\n    \"neutral_realistic\": {\n        \"wb\": np.array([1.00, 1.00, 1.00]),\n        \"ccm\": np.eye(3)\n    },\n    \"vintage_film\": {\n        \"wb\": np.array([1.05, 1.00, 0.95]),\n        \"ccm\": np.array([\n            [ 0.95,  0.05,  0.00],\n            [ 0.00,  1.00,  0.00],\n            [ 0.00, -0.05,  1.05],\n        ])\n    },\n     \"kodak_vision3_arri_alexa\": {\n        \"wb\": np.array([1.05, 1.00, 0.95]),\n        \"ccm\": np.array([\n            [1.10, -0.02, -0.08],\n            [-0.01, 1.05, -0.04],\n            [0.00, -0.05, 1.08]\n        ])\n    },\n    \"filmic_natural_sony_venice_red\": {\n        \"wb\": np.array([1.00, 1.00, 1.00]),\n        \"ccm\": np.array([\n            [1.08, -0.05, -0.03],\n            [-0.03, 1.05, -0.02],\n            [-0.02, -0.05, 1.10]\n        ])\n    },\n    \"cinematic_teal_orange\": {\n        \"wb\": np.array([1.10, 0.95, 0.90]),\n        \"ccm\": np.array([\n            [1.15, -0.10, -0.05],\n            [-0.10, 1.05, -0.10],\n            [-0.05, -0.05, 1.20]\n        ])\n    },\n    \"fujifilm_classic_chrome\": {\n        \"wb\": np.array([1.00, 1.02, 0.98]),\n        \"ccm\": np.array([\n            [1.05, -0.02, -0.03],\n            [-0.02, 1.03, -0.01],\n            [0.00, -0.01, 1.02]\n        ])\n    },\n    \"cool_balanced_modern\": {\n        \"wb\": np.array([0.99, 1.00, 1.05]),\n        \"ccm\": np.array([\n            [1.10, -0.04, -0.04],  \n            [-0.03, 1.08, -0.02],  \n            [-0.02, -0.04, 1.12]  \n        ])\n    },\n    \"photorealistic_balanced\": {\n    \"wb\": np.array([1.03, 1.00, 0.97]),\n    \"ccm\": np.array([\n        [1.08, -0.03, -0.05],\n        [-0.02, 1.05, -0.03],\n        [-0.01, -0.04, 1.10]\n    ])\n},\n\"realistic_nature_boost\": {\n    \"wb\": np.array([1.02, 1.00, 0.98]),\n    \"ccm\": np.array([\n        [1.06, -0.02, -0.04],\n        [-0.02, 1.05, -0.03],\n        [-0.01, -0.02, 1.08]\n    ])\n},\n\"urban_neutral_film\": {\n    \"wb\": np.array([1.00, 1.00, 1.00]),\n    \"ccm\": np.array([\n        [1.05, -0.01, -0.04],\n        [-0.01, 1.06, -0.03],\n        [-0.02, -0.02, 1.07]\n    ])\n},\n\"subtle_filmic_cool\": {\n    \"wb\": np.array([1.00, 0.98, 1.02]),\n    \"ccm\": np.array([\n        [1.04, -0.02, -0.02],\n        [-0.01, 1.03, -0.01],\n        [-0.01, -0.02, 1.05]\n    ])\n}\n    \n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T22:07:37.278562Z","iopub.execute_input":"2025-07-15T22:07:37.278967Z","iopub.status.idle":"2025-07-15T22:07:37.333098Z","shell.execute_reply.started":"2025-07-15T22:07:37.278927Z","shell.execute_reply":"2025-07-15T22:07:37.332099Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# -------------------------\n# Helper Functions\n# -------------------------\n\ndef white_balance(image, wb):\n    # Per-channel multiplication for white balance\n    wb_img = image * wb.reshape((1, 1, 3))\n    return np.clip(wb_img, 0.0, 1.0)\n\ndef color_correction(image, ccm):\n    # Reshape image to (H*W, 3), apply CCM, then reshape back\n    h, w, _ = image.shape\n    corrected = np.dot(image.reshape(-1, 3), ccm.T)\n    return np.clip(corrected.reshape(h, w, 3), 0.0, 1.0)\n\ndef adjust_contrast_saturation(image, contrast=1.2, saturation=1.2):\n    # Convert to HSV, adjust saturation and value (contrast), then convert back\n    image_8bit = np.uint8(np.clip(image * 255, 0, 255))\n    hsv = cv2.cvtColor(image_8bit, cv2.COLOR_RGB2HSV).astype(np.float32)\n    hsv[..., 1] *= saturation  # Saturation channel\n    hsv[..., 2] = np.clip(hsv[..., 2] * contrast, 0, 255)  # Value channel\n    hsv = np.clip(hsv, 0, 255).astype(np.uint8)\n    result = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n    return result.astype(np.float32) / 255.0\n\ndef shadow_correction(image):\n    # Convert to LAB and apply CLAHE on the L channel for shadow recovery\n    image_8bit = np.uint8(np.clip(image * 255, 0, 255))\n    lab = cv2.cvtColor(image_8bit, cv2.COLOR_RGB2LAB)\n    l, a, b = cv2.split(lab)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n    l_eq = clahe.apply(l)\n    lab_eq = cv2.merge((l_eq, a, b))\n    corrected = cv2.cvtColor(lab_eq, cv2.COLOR_LAB2RGB)\n    return corrected.astype(np.float32) / 255.0\n\ndef lens_blur(image, kernel_size=3):\n    # Apply a slight Gaussian blur to mimic lens imperfections.\n    # kernel_size should be an odd number.\n    blurred = cv2.GaussianBlur(image, (kernel_size, kernel_size), sigmaX=0.5)\n    return blurred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T22:07:41.632631Z","iopub.execute_input":"2025-07-15T22:07:41.632899Z","iopub.status.idle":"2025-07-15T22:07:41.640375Z","shell.execute_reply.started":"2025-07-15T22:07:41.632876Z","shell.execute_reply":"2025-07-15T22:07:41.639535Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nclass CNILUT(nn.Module):\n    \"\"\"\n    Simple residual coordinate-based neural network for fitting 3D LUTs\n    Official code: https://github.com/mv-lab/nilut\n    \"\"\"\n    def __init__(self, in_features=3, hidden_features=256, hidden_layers=3, out_features=3, styles=3, res=True):\n        super().__init__()\n        \n        self.styles = styles\n        self.res = res\n        self.net = []\n        self.net.append(nn.Linear(in_features+styles, hidden_features))\n        self.net.append(nn.ReLU())\n        \n        for _ in range(hidden_layers):\n            self.net.append(nn.Linear(hidden_features, hidden_features))\n            self.net.append(nn.Tanh())\n        \n        self.net.append(nn.Linear(hidden_features, out_features))\n        if not self.res:\n            self.net.append(torch.nn.Sigmoid())\n        \n        self.net = nn.Sequential(*self.net)\n    \n    def forward(self, intensity):\n        output = self.net(intensity)\n        if self.res:\n            output = output + intensity[:,:self.styles]\n            output = torch.clamp(output, 0.,1.)\n        \n        return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T22:07:43.990656Z","iopub.execute_input":"2025-07-15T22:07:43.990975Z","iopub.status.idle":"2025-07-15T22:07:47.207664Z","shell.execute_reply.started":"2025-07-15T22:07:43.990945Z","shell.execute_reply":"2025-07-15T22:07:47.207020Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"PATH = \"/kaggle/input/nilut-data/nilutx3style.pt\"\nlut_model  = CNILUT(in_features=3, out_features=3, hidden_features=256, hidden_layers=2, styles=3, res=True)\nlut_model.load_state_dict(torch.load(PATH, weights_only=True)['model'])\nlut_model.cuda()\nlut_model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T22:07:47.402653Z","iopub.execute_input":"2025-07-15T22:07:47.402966Z","iopub.status.idle":"2025-07-15T22:07:47.841707Z","shell.execute_reply.started":"2025-07-15T22:07:47.402944Z","shell.execute_reply":"2025-07-15T22:07:47.840845Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"CNILUT(\n  (net): Sequential(\n    (0): Linear(in_features=6, out_features=256, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=256, out_features=256, bias=True)\n    (3): Tanh()\n    (4): Linear(in_features=256, out_features=256, bias=True)\n    (5): Tanh()\n    (6): Linear(in_features=256, out_features=3, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"def run_nilut (image, style=1):\n    image_tensor = torch.from_numpy(image)\n    image_tensor = image_tensor.reshape((image_tensor.shape[0]*image_tensor.shape[1],3)) # [hw, 3]\n\n    style_vector = np.zeros(3).astype(np.float32)\n    style_vector[style] = 1.\n    style_vector    = torch.from_numpy(style_vector)\n    style_vector_re = style_vector.repeat(image_tensor.shape[0]).view(image_tensor.shape[0],3)\n\n    img = torch.cat([image_tensor,style_vector_re], dim=-1)\n\n    with torch.no_grad():\n        out = lut_model(img.cuda())\n\n    np_out  = out.cpu().view(image.shape[0],image.shape[1],3).detach().numpy().astype(np.float32)\n    return np_out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T22:07:49.995790Z","iopub.execute_input":"2025-07-15T22:07:49.996121Z","iopub.status.idle":"2025-07-15T22:07:50.001598Z","shell.execute_reply.started":"2025-07-15T22:07:49.996091Z","shell.execute_reply":"2025-07-15T22:07:50.000723Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# -------------------------\n# Enhanced Realism + Plan B Pipeline\n# -------------------------\n\ndef add_vignette(image, strength=0.1):\n    h, w = image.shape[:2]\n    Y, X = np.ogrid[:h, :w]\n    center = (h/2, w/2)\n    distance = np.sqrt((X - center[1])**2 + (Y - center[0])**2)\n    max_dist = np.sqrt(center[0]**2 + center[1]**2)\n    mask = 1 - strength * (distance / max_dist)\n    mask = np.clip(mask, 0.8, 1)\n    return (image * mask[..., np.newaxis])\n\ndef sharpen(image, amount=0.3):\n    blurred = cv2.GaussianBlur(image, (0, 0), 3)\n    return np.clip(image + amount * (image - blurred), 0, 1)\n\ndef slight_desaturation(image, factor=0.95):\n    image_8bit = np.uint8(np.clip(image * 255, 0, 255))\n    hsv = cv2.cvtColor(image_8bit, cv2.COLOR_RGB2HSV).astype(np.float32)\n    hsv[...,1] *= factor\n    hsv = np.clip(hsv, 0, 255).astype(np.uint8)\n    image = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n    return image.astype(np.float32) / 255.0\n\ndef enhanced_pipeline(image, wb, ccm, \n                      gamma_val=2.2, \n                      contrast=1.2, \n                      saturation=1.2,\n                      blur_kernel=3, \n                      lut=None,\n                      apply_vignette=True,\n                      apply_sharpen=True,\n                      apply_desaturation=True):\n    \"\"\"\n    Full enhanced ISP pipeline for photorealism (merged with Plan B).\n    \"\"\"\n\n    # 0. Optional: Add tiny random WB jitter (for diversity)\n    wb = wb + np.random.normal(0, 0.005, wb.shape)\n    wb = np.clip(wb, 0.95, 1.05)\n\n    # 1. White Balance\n    image = white_balance(image, wb)\n    \n    # 2. Color Correction with CCM\n    image = color_correction(image, ccm)\n    \n    # 3. Adaptive Gamma Correction\n    avg_brightness = image.mean()\n    gamma_val = 1.8 if avg_brightness > 0.5 else 2.2\n    image = np.clip(np.power(image, 1.0 / gamma_val), 0, 1)\n    \n    # 4. Contrast and Saturation Adjustment\n    image = adjust_contrast_saturation(image, contrast=contrast, saturation=saturation)\n    \n    # 5. Shadow Correction (via LAB CLAHE)\n    image = shadow_correction(image)\n    \n    # 6. Optional Lens Blur (simulate optical imperfections)\n    image = lens_blur(image, kernel_size=blur_kernel)\n    \n    # 7. Neural LUT-based Color Mapping\n    image = run_nilut(image, style=1)\n\n    # 8. Vignette (optional)\n    if apply_vignette:\n        image = add_vignette(image, strength=0.1)\n\n    # 9. Sharpening (optional)\n    if apply_sharpen:\n        image = sharpen(image, amount=0.1)   # lighter sharpen for final polish\n\n    # 10. Slight Desaturation (optional for realism)\n    if apply_desaturation:\n        image = slight_desaturation(image, factor=0.95)\n\n    return image\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T22:07:53.388764Z","iopub.execute_input":"2025-07-15T22:07:53.389101Z","iopub.status.idle":"2025-07-15T22:07:53.398636Z","shell.execute_reply.started":"2025-07-15T22:07:53.389066Z","shell.execute_reply":"2025-07-15T22:07:53.397727Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nfrom tqdm import tqdm\nfrom glob import glob\n\ndef process_selected_folders(root_folder, selected_folders, output_folder,\n                             wb, ccm, gamma_val=2.2, contrast=1.2, saturation=1.2,\n                             blur_kernel=3, lut=None):\n\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n\n    image_files = []\n\n    # Go through only selected folders and find images recursively\n    for folder in selected_folders:\n        folder_path = os.path.join(root_folder, folder)\n        found = glob(os.path.join(folder_path, \"**\", \"*.*\"), recursive=True)\n        image_files.extend(found)\n\n    # Filter for image extensions\n    image_files = [f for f in image_files if f.lower().endswith(('jpg', 'png', 'jpeg'))]\n\n    print(\"📁 Selected folders:\", selected_folders)\n    print(\"🔍 Total image files found:\", len(image_files))\n\n    for image_path in tqdm(image_files):\n        try:\n            image = load_img(image_path)\n\n            output_img = enhanced_pipeline(image, wb, ccm,\n                                           gamma_val=gamma_val,\n                                           contrast=contrast,\n                                           saturation=saturation,\n                                           blur_kernel=blur_kernel,\n                                           lut=lut)\n\n            output_display = np.uint8(np.clip(output_img * 255, 0, 255))\n            filename = os.path.basename(image_path)\n            output_path = os.path.join(output_folder, filename)\n            cv2.imwrite(output_path, cv2.cvtColor(output_display, cv2.COLOR_RGB2BGR))\n\n        except Exception as e:\n            print(f\"❌ Failed to process {image_path}: {e}\")\n\n    print(f\"\\n✅ Done! Processed {len(image_files)} images and saved to: {output_folder}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T22:08:01.545825Z","iopub.execute_input":"2025-07-15T22:08:01.546199Z","iopub.status.idle":"2025-07-15T22:08:01.553567Z","shell.execute_reply.started":"2025-07-15T22:08:01.546167Z","shell.execute_reply":"2025-07-15T22:08:01.552810Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"Nilut","metadata":{}},{"cell_type":"code","source":"grade = color_grades[\"cool_balanced_modern\"]\nwb = grade[\"wb\"]\nccm = grade[\"ccm\"]\nselected_folders = [f\"{str(i).zfill(2)}_images\" for i in range(10, 11)]  # 01_images to 05_images\n\n\nroot_folder = \"/kaggle/input/playing-for-data\"\noutput_folder = \"/kaggle/working/isp_nilut_output_10_images\"\n\nprocess_selected_folders(root_folder, selected_folders, output_folder,\n                         wb, ccm, gamma_val=1.8, contrast=1.1, saturation=1.15,\n                         blur_kernel=1, lut=None)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T22:08:06.791074Z","iopub.execute_input":"2025-07-15T22:08:06.791389Z","iopub.status.idle":"2025-07-15T22:46:44.670472Z","shell.execute_reply.started":"2025-07-15T22:08:06.791362Z","shell.execute_reply":"2025-07-15T22:46:44.669568Z"}},"outputs":[{"name":"stdout","text":"📁 Selected folders: ['10_images']\n🔍 Total image files found: 2466\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2466/2466 [38:34<00:00,  1.07it/s]","output_type":"stream"},{"name":"stdout","text":"\n✅ Done! Processed 2466 images and saved to: /kaggle/working/isp_nilut_output_10_images\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!pip install clean-fid\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T22:46:44.671730Z","iopub.execute_input":"2025-07-15T22:46:44.672106Z","iopub.status.idle":"2025-07-15T22:46:49.023630Z","shell.execute_reply.started":"2025-07-15T22:46:44.672070Z","shell.execute_reply":"2025-07-15T22:46:49.022774Z"}},"outputs":[{"name":"stdout","text":"Collecting clean-fid\n  Downloading clean_fid-0.1.35-py3-none-any.whl.metadata (36 kB)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clean-fid) (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clean-fid) (0.20.1+cu121)\nRequirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from clean-fid) (1.26.4)\nRequirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from clean-fid) (1.13.1)\nRequirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.10/dist-packages (from clean-fid) (4.67.1)\nRequirement already satisfied: pillow>=8.1 in /usr/local/lib/python3.10/dist-packages (from clean-fid) (11.0.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from clean-fid) (2.32.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.3->clean-fid) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.3->clean-fid) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.3->clean-fid) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.3->clean-fid) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.3->clean-fid) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.14.3->clean-fid) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->clean-fid) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->clean-fid) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->clean-fid) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->clean-fid) (2024.12.14)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clean-fid) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clean-fid) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clean-fid) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clean-fid) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clean-fid) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->clean-fid) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->clean-fid) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clean-fid) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.3->clean-fid) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.14.3->clean-fid) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.3->clean-fid) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.14.3->clean-fid) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.14.3->clean-fid) (2024.2.0)\nDownloading clean_fid-0.1.35-py3-none-any.whl (26 kB)\nInstalling collected packages: clean-fid\nSuccessfully installed clean-fid-0.1.35\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from cleanfid import fid\n\n\nfdir1 = \"/kaggle/input/photorealism-data/ours_Cityscape\"   # Cityscapes (Ground Truth)\nfdir2 = \"/kaggle/input/playing-for-data/01_images/images\"  # change this\n\n# FID\nscore_fid = fid.compute_fid(fdir1, fdir2)\nprint(f\"FID Score: {score_fid}\")\n\nprint(\"cool_balanced_modern contrast= 1.1, saturation=1.15 and added vignette sharpen and slight jitter and final boost for desaturation\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T22:46:49.025089Z","iopub.execute_input":"2025-07-15T22:46:49.025369Z","iopub.status.idle":"2025-07-15T22:52:20.595292Z","shell.execute_reply.started":"2025-07-15T22:46:49.025342Z","shell.execute_reply":"2025-07-15T22:52:20.594348Z"}},"outputs":[{"name":"stdout","text":"compute FID between two folders\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Found 19252 images in the folder /kaggle/input/photorealism-data/ours_Cityscape\n","output_type":"stream"},{"name":"stderr","text":"FID ours_Cityscape : 100%|██████████| 602/602 [03:04<00:00,  3.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Found 2500 images in the folder /kaggle/input/playing-for-data/01_images/images\n","output_type":"stream"},{"name":"stderr","text":"FID images : 100%|██████████| 79/79 [01:48<00:00,  1.37s/it]\n","output_type":"stream"},{"name":"stdout","text":"FID Score: 54.14864577416438\ncool_balanced_modern contrast= 1.1, saturation=1.15 and added vignette sharpen and slight jitter and final boost for desaturation\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from cleanfid import fid\n\n\nscore_kid = fid.compute_kid(fdir1, fdir2)\nprint(f\"KID Score: {score_kid * 1000:.3f}\")  #Multiply by 100 for proper reporting","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T22:52:20.596368Z","iopub.execute_input":"2025-07-15T22:52:20.596724Z","iopub.status.idle":"2025-07-15T22:57:20.524404Z","shell.execute_reply.started":"2025-07-15T22:52:20.596700Z","shell.execute_reply":"2025-07-15T22:57:20.523469Z"}},"outputs":[{"name":"stdout","text":"compute KID between two folders\nFound 19252 images in the folder /kaggle/input/photorealism-data/ours_Cityscape\n","output_type":"stream"},{"name":"stderr","text":"KID ours_Cityscape : 100%|██████████| 602/602 [03:02<00:00,  3.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Found 2500 images in the folder /kaggle/input/playing-for-data/01_images/images\n","output_type":"stream"},{"name":"stderr","text":"KID images : 100%|██████████| 79/79 [01:45<00:00,  1.34s/it]\n","output_type":"stream"},{"name":"stdout","text":"KID Score: 58.996\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}